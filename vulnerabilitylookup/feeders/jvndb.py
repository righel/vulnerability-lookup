#!/usr/bin/env python3

from __future__ import annotations

import json

from datetime import datetime
import hashlib
from pathlib import Path
from typing import Callable

import orjson
import requests
import xmltodict

from ..helpers import fromisoformat_wrapper

from .abstract_feeder import AbstractFeeder


class JVNDB(AbstractFeeder):
    def __init__(self) -> None:
        super().__init__(Path(__file__).stem)
        self.files_storage = Path(__file__).parent / self.name
        self.files_storage.mkdir(parents=True, exist_ok=True)

    def _fetch_feeds(self) -> list[Path]:
        all_files = []
        base_url = "https://jvndb.jvn.jp/en/rss/years/jvndb_{year}.rdf"
        for year in range(2002, datetime.now().year + 1):
            to_fetch = base_url.format(year=year)
            with (self.files_storage / f'jvndb_{year}.rdf').open('w') as f:
                response = requests.get(to_fetch, timeout=15)
                f.write(response.text)
            all_files.append(self.files_storage / f'jvndb_{year}.rdf')

        # NOTE: this file must always be the last one in the list.
        with (self.files_storage / 'jvndb_updates.rdf').open('w') as f:
            response = requests.get('https://jvndb.jvn.jp/en/rss/jvndb.rdf', timeout=15)
            f.write(response.text)
        all_files.append(self.files_storage / 'jvndb_updates.rdf')
        return all_files

    def update(self, stop: Callable[..., bool]) -> bool:
        if (self.files_storage / 'jvndb.sha256').exists():
            with (self.files_storage / 'jvndb.sha256').open() as f:
                already_loaded = json.load(f)
        else:
            already_loaded = {}

        got_update = False
        for localfile in self._fetch_feeds():
            if not localfile.exists():
                # No file with that name
                self.logger.warning(f'{localfile} does not exist.')
                continue

            with localfile.open('rb') as f:
                sha256 = hashlib.sha256(f.read()).hexdigest()
            if already_loaded.get(localfile.name) == sha256:
                # The file was already imported
                continue

            already_loaded[localfile.name] = sha256
            got_update = True

            p = self.storage.pipeline()
            jvndbids: dict[str, float] = {}
            with localfile.open() as f:
                feed = xmltodict.parse(f.read())
                # NOTE: for the yearly dumps, the order doesn't matters.
                # For the updates, it does. We need to load from the end of the list
                if not feed.get('rdf:RDF'):
                    continue
                entries = feed['rdf:RDF'].get('item')
                if not entries:
                    continue
                if isinstance(entries, dict):
                    # only one entry
                    entries = [entries]
                else:
                    entries = reversed(entries)
                for vuln in entries:
                    vuln_id = vuln['sec:identifier'].lower()

                    if 'dcterms:modified' in vuln:
                        last_modified = fromisoformat_wrapper(vuln['dcterms:modified'])
                    elif 'dcterms:issued' in vuln:
                        last_modified = fromisoformat_wrapper(vuln['dcterms:issued'])
                    elif 'date' in vuln:
                        last_modified = fromisoformat_wrapper(vuln['date'])
                    else:
                        self.logger.warning(f'no date found for {vuln_id}')
                        continue
                    jvndbids[vuln_id] = last_modified.timestamp()
                    if 'sec:references' in vuln:
                        if isinstance(vuln['sec:references'], dict):
                            # only one reference
                            refs = [vuln['sec:references']]
                        else:
                            refs = vuln['sec:references']
                        for ref in refs:
                            if ref.get('@source') in ['CVE', 'NVD']:
                                r = ref['@id'].lower()
                                p.sadd(f"{vuln_id}:link", r)
                                p.sadd(f'{r}:link', vuln_id)
                            elif ref.get('@source'):
                                self.logger.warning(f'Unknown reference source: {ref.get("@source")} {ref}')
                    if 'sec:cpe' in vuln:
                        if isinstance(vuln['sec:cpe'], dict):
                            # only one cpe
                            cpes = [vuln['sec:cpe']]
                        else:
                            cpes = vuln['sec:cpe']
                        for cpe in cpes:
                            vendor = cpe['@vendor'].strip().lower()
                            product = cpe['@product'].strip().lower()
                            p.sadd('vendors', vendor)
                            p.sadd(f'{vendor}:products', product)
                            p.sadd(f'{vendor}:vulnerabilities', vuln_id)
                            p.sadd(f'{vendor}:{product}:vulnerabilities', vuln_id)
                    vuln_bytes = orjson.dumps(vuln)
                    p.set(vuln_id, vuln_bytes)
                    # Publish the vulnerability
                    self.publish(vuln_bytes)
                p.zadd(f'index:{self.name}', jvndbids)  # type: ignore
                p.zadd('index', jvndbids)  # type: ignore
                p.execute()

        if got_update:
            self.storage.hset('last_updates', mapping={self.name: last_modified.timestamp()})
            self.logger.info('Import done.')
            # Processed all the files, update the hash file
            with (self.files_storage / 'jvndb.sha256').open('w') as f:
                json.dump(already_loaded, f)
            return True
        else:
            self.logger.info('Nothing new to import.')
            return False
