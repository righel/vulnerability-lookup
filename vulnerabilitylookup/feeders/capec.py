from __future__ import annotations

import logging
import requests
import zipfile

from datetime import datetime, date
from io import BytesIO
from pathlib import Path
from typing import Callable

import orjson
import xmltodict

from ..helpers import fromisoformat_wrapper

from .abstract_feeder import AbstractFeeder


class CAPEC(AbstractFeeder):
    def __init__(self) -> None:
        super().__init__(Path(__file__).stem)
        self.url = 'https://capec.mitre.org/data/archive/capec_latest.zip'

    def _load_logging_config(self) -> None:
        cur_path = Path(__file__)
        if not (cur_path.parent / f'{cur_path.stem}_logging.json').exists():
            return
        with (cur_path.parent / f'{cur_path.stem}_logging.json').open() as f:
            log_config = orjson.loads(f.read())
        logging.config.dictConfig(log_config)

    def update(self, stop: Callable[..., bool]) -> bool:
        # NOTE: Not checking stop in this method as the main thing that will lock is the request
        # And this one has a timeout set to 300 seconds
        r = requests.get(self.url, timeout=300)
        try:
            r.raise_for_status()
        except Exception as e:
            self.logger.debug(f'Failed to fetch: {e}')
            return False

        with zipfile.ZipFile(BytesIO(r.content)) as z:
            for fn in z.filelist:
                if fn.filename.startswith('capec'):
                    with z.open(fn.filename) as f:
                        capecs = xmltodict.parse(f.read())
                    break

        last_feed_update = datetime.combine(date.fromisoformat(capecs['Attack_Pattern_Catalog']['@Date']), datetime.min.time())
        if _last_update := self.storage.hget('last_updates', self.name):
            last_update = fromisoformat_wrapper(_last_update.decode())
            if last_update >= last_feed_update:
                self.logger.info('No updates.')
                return False

        p = self.storage.pipeline()
        capec_ids: dict[str, float] = {}
        for capec in capecs['Attack_Pattern_Catalog']['Attack_Patterns']['Attack_Pattern']:
            capec_id = f"capec-{capec['@ID']}"
            if 'Content_History' in capec and 'Modification' in capec['Content_History']:
                if isinstance(capec['Content_History']['Modification'], dict):
                    last_modified = datetime.combine(date.fromisoformat(capec['Content_History']['Modification']['Modification_Date']), datetime.min.time())
                else:
                    last_modified = datetime.combine(date.fromisoformat(capec['Content_History']['Modification'][-1]['Modification_Date']), datetime.min.time())
            else:
                last_modified = datetime.combine(date.fromisoformat(capec['Content_History']['Submission']['Submission_Date']), datetime.min.time())

            capec_ids[capec_id] = last_modified.timestamp()
            p.set(capec_id, orjson.dumps(capec))
        p.zadd(f'index:{self.name}', capec_ids)  # type: ignore
        p.zadd('index', capec_ids)  # type: ignore
        p.execute()

        self.storage.hset('last_updates', mapping={self.name: last_feed_update.isoformat()})
        self.logger.info('Import done.')
        return True
