from __future__ import annotations

import re

import orjson
from pathlib import Path
from typing import Callable

from ..helpers import fromisoformat_wrapper

from .abstract_feeder import AbstractFeeder


class Github(AbstractFeeder):
    def __init__(self) -> None:
        super().__init__(Path(__file__).stem)

        self.init_git_repo()

    def update(self, stop: Callable[..., bool]) -> bool:
        self.git.remotes.origin.pull('main')

        paths_to_import: set[Path] = set()
        if _last_update := self.storage.hget('last_updates', self.name):
            _last_update_str = _last_update.decode()
            if _last_update_str == self.git.head.commit.hexsha:
                # No changes
                self.logger.info('No new commit.')
                return False
            for commit in self.git.iter_commits(f'{_last_update_str}...HEAD'):
                for line in self.git.git.show(commit.hexsha, name_only=True).split('\n'):
                    if not line.endswith('.json'):
                        continue
                    p_path = self.path_to_repo / Path(line)
                    if p_path.exists() and re.match('GHSA(-[23456789cfghjmpqrvwx]{4}){3}', p_path.stem):
                        paths_to_import.add(p_path)

        else:
            # First run, get all files
            for year_dir in (sorted((self.path_to_repo / 'advisories' / 'github-reviewed').iterdir())
                             + sorted((self.path_to_repo / 'advisories' / 'unreviewed').iterdir())):
                if not year_dir.name.isdigit():
                    continue
                for month_dir in sorted(year_dir.iterdir()):
                    if not month_dir.name.isdigit():
                        continue
                    for entry in sorted(month_dir.iterdir()):
                        if not re.match('GHSA(-[23456789cfghjmpqrvwx]{4}){3}', entry.stem):
                            continue
                        if (entry / f'{entry.stem}.json').exists():
                            paths_to_import.add(entry / f'{entry.stem}.json')

        if not paths_to_import:
            return False

        import_complete: bool = True
        p = self.storage.pipeline()
        gsids: dict[str, float] = {}
        vuln_to_push = []
        for path in paths_to_import:
            # Store all cves individually
            with path.open() as vuln_entry:
                vuln_str = vuln_entry.read()
                vuln_to_push.append(vuln_str)
                vuln = orjson.loads(vuln_str)
                modified = fromisoformat_wrapper(vuln['modified'])
                vuln_id = path.stem.lower()
                gsids[vuln_id] = modified.timestamp()
                if 'aliases' in vuln and vuln.get('aliases'):
                    for alias in vuln.get('aliases'):
                        a = alias.lower()
                        p.sadd(f'{vuln_id}:link', a)
                        p.sadd(f'{a}:link', vuln_id)
                p.set(vuln_id, orjson.dumps(vuln))
            if len(gsids) > 1000:
                # Avoid a massive execute on first import
                p.zadd(f'index:{self.name}', gsids)  # type: ignore
                p.zadd('index', gsids)  # type: ignore
                p.execute()

                # Push the vulnerabilities
                self.publish(vuln_to_push)

                # reset pipeline
                p = self.storage.pipeline()
                gsids = {}

                # Reset the list vulnerabilities to push
                vuln_to_push = []

            if stop():
                import_complete = False
                self.logger.info('Shutdown requested. Stopping import.')
                break

        if gsids:
            # remaining entries
            p.zadd(f'index:{self.name}', gsids)  # type: ignore
            p.zadd('index', gsids)  # type: ignore
            p.execute()

            # Push the vulnerabilities
            self.publish(vuln_to_push)
            vuln_to_push = []

        if import_complete:
            self.storage.hset('last_updates', mapping={self.name: self.git.head.commit.hexsha})
            self.logger.info('Import done.')
            return True

        return False
