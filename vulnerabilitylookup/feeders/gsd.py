from __future__ import annotations

from pathlib import Path
from typing import Callable

import orjson

from ..helpers import fromisoformat_wrapper

from .abstract_feeder import AbstractFeeder


class GSD(AbstractFeeder):
    def __init__(self) -> None:
        super().__init__(Path(__file__).stem)

        self.init_git_repo()

    def update(self, stop: Callable[..., bool]) -> bool:

        self.git.remotes.origin.pull('main')

        paths_to_import: set[Path] = set()
        if _last_update := self.storage.hget('last_updates', self.name):
            _last_update_str = _last_update.decode()
            if _last_update_str == self.git.head.commit.hexsha:
                # No changes
                self.logger.info('No new commit.')
                return False
            for commit in self.git.iter_commits(f'{_last_update_str}...HEAD'):
                for line in self.git.git.show(commit.hexsha, name_only=True).split('\n'):
                    if not line.endswith('.json'):
                        continue
                    p_path = self.path_to_repo / Path(line)
                    if p_path.exists() and p_path.parent.name.endswith('xxx') and p_path.parent.parent.name.isdigit():
                        paths_to_import.add(p_path)
        else:
            # First run, get all files
            for year_dir in sorted(self.path_to_repo.iterdir()):
                if not year_dir.name.isdigit():
                    continue
                for vuln_dir in sorted(year_dir.iterdir()):
                    if not vuln_dir.name.endswith('xxx'):
                        continue
                    for entry in sorted(vuln_dir.iterdir()):
                        if entry.suffix != '.json':
                            continue
                        paths_to_import.add(entry)

        if not paths_to_import:
            self.logger.info('Nothing new to import.')
            return False

        import_complete: bool = True
        p = self.storage.pipeline()
        gsids: dict[str, float] = {}
        vuln_to_push = []
        for path in paths_to_import:
            needs_lastmodified_from_git = False
            last_modified = None
            # Store all cves individually
            with path.open() as vuln_entry:
                vuln_str = vuln_entry.read()
                vuln_to_push.append(vuln_str)
                vuln = orjson.loads(vuln_str)
                # NOTE 2024-01-11: Just another rewrite of the GSD format...
                # as of today, 3 files do not have either a "GSD" or a "gsd" key, go figure.

                # Note 2024-01-16: aliases can be at multiple places in the file.
                aliases = set()
                if gsd := vuln.get('gsd'):
                    if osvSchema := gsd.get('osvSchema'):
                        # That's our best case scenario, if there is an osvSchema, we can get the id and modified time.
                        last_modified = fromisoformat_wrapper(osvSchema['modified'])
                        if _aliases := osvSchema.get('aliases'):
                            aliases.update(_aliases)
                    elif modified := gsd.get('modified'):
                        last_modified = fromisoformat_wrapper(modified)

                # NOTE 2023-09-06: the GSD file format doesn't contains a modified timestamp
                # but the sources seem to kinda have one, most of the time, maybe.
                if namespaces := vuln.get('namespaces'):

                    if nvd := namespaces.get('nvd.nist.gov'):
                        if not last_modified and 'lastModified' in nvd:
                            try:
                                last_modified = fromisoformat_wrapper(nvd['lastModified'])
                            except ValueError as e:
                                # nothing important, just a broken date
                                self.logger.warning(f'Unable to get modified date: {e}')
                                needs_lastmodified_from_git = True

                        if 'id' in nvd:
                            aliases.add(nvd['id'])
                    if gitlab := namespaces.get('gitlab.com'):
                        if not last_modified:
                            try:
                                last_modified = fromisoformat_wrapper(gitlab['advisories'][0]['date'])
                            except ValueError as e:
                                # nothing important, just a broken date
                                self.logger.warning(f'Unable to get modified date: {e}')
                                needs_lastmodified_from_git = True
                    else:
                        # 2024-01-11: just try to a find midified key somewhere in the namespaces, YOLO
                        for ns in vuln['namespaces'].values():
                            if 'modified' in ns:
                                last_modified = fromisoformat_wrapper(ns['modified'])
                                break

                    if cve := namespaces.get('cve.org'):
                        if not last_modified:
                            if cve['CVE_data_meta']['STATE'] == 'RESERVED':
                                # reserved ID
                                needs_lastmodified_from_git = True
                        if 'CVE_data_meta' in cve and 'ID' in cve['CVE_data_meta']:
                            aliases.add(cve['CVE_data_meta']['ID'])

                if osv := vuln.get('OSV'):
                    if not last_modified:
                        if 'modified' in osv:
                            last_modified = fromisoformat_wrapper(osv['modified'])
                        elif 'withdrawn' in vuln['OSV']:
                            last_modified = fromisoformat_wrapper(osv['withdrawn'])

                if gsd_old := vuln.get('GSD'):
                    # NOTE: no idea if it is a thing or not, but it was there, so maybe.
                    if not last_modified:
                        if 'modified' in gsd_old:
                            last_modified = fromisoformat_wrapper(gsd_old['modified'])
                        elif 'withdrawn' in gsd_old:
                            last_modified = fromisoformat_wrapper(gsd_old['withdrawn'])
                        else:
                            needs_lastmodified_from_git = True

                    if _alias := gsd_old.get('alias'):
                        aliases.add(_alias)

                if not last_modified:
                    if not needs_lastmodified_from_git:
                        self.logger.warning(f'Unable to process {path}, please have a look yourself, good luck!')
                        continue
                    self.logger.info(f'Unable to find modified time in {path}, using git.')
                    # NOTE old approach: there is no indication when the entry was last updated in the json,
                    # using the last time that file was commited
                    # It is slow as hell, but that's the best we can do.

                    commit = next(self.git.iter_commits(max_count=1, paths=path))
                    last_modified = commit.committed_datetime

                vuln_id = path.stem.lower()
                if not aliases:
                    # The vuln id >= 1000xxx are GSD only
                    if int(vuln_id.rsplit('-', 1)[-1]) < 1000000:
                        self.logger.info(f'No aliases for {path}.')

                for alias in {a.lower() for a in aliases}:
                    p.sadd(f'{vuln_id}:link', alias)
                    p.sadd(f'{alias}:link', vuln_id)
                p.set(vuln_id, orjson.dumps(vuln))
                gsids[vuln_id] = last_modified.timestamp()

            if len(gsids) > 1000:
                # Avoid a massive execute on first import
                p.zadd(f'index:{self.name}', gsids)  # type: ignore
                p.zadd('index', gsids)  # type: ignore
                p.execute()

                # Push the vulnerabilities
                self.publish(vuln_to_push)

                # reset pipeline
                p = self.storage.pipeline()
                gsids = {}

                # Reset the list vulnerabilities to push
                vuln_to_push = []

            if stop():
                self.logger.info('Shutdown requested. Stopping import.')
                import_complete = False

        if gsids:
            # remaining entries
            p.zadd(f'index:{self.name}', gsids)  # type: ignore
            p.zadd('index', gsids)  # type: ignore
            p.execute()

            # Push the vulnerabilities
            self.publish(vuln_to_push)
            vuln_to_push = []

        if import_complete:
            self.storage.hset('last_updates', mapping={self.name: self.git.head.commit.hexsha})
            self.logger.info('Import done.')
            return True

        return False
