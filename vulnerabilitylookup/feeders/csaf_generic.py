from __future__ import annotations

import json
import os
import re
import time

import orjson

from datetime import datetime
from pathlib import Path
from subprocess import Popen
from typing import Callable

from ..default import get_config, get_homedir
from ..helpers import fromisoformat_wrapper

from .abstract_feeder import AbstractFeeder


class CSAFGeneric(AbstractFeeder):
    def __init__(self, feeder_name: str) -> None:
        super().__init__(feeder_name)

        csaf_downloader = get_config('generic', 'csaf_downloader_path')
        if not csaf_downloader or not os.path.exists(csaf_downloader):
            self.logger.error('A CSAF downloader is required for this feeder')
            raise FileNotFoundError('CSAF downloader not found')

        if not hasattr(self, 'csaf_metadata'):
            self.logger.error('The URL to the metadata file is required for this feeder')
            raise Exception('Link to the metadata CSAF repository is missing.')

        if not hasattr(self, 'file_pattern'):
            self.file_pattern = '.*json$'
        self.init_csaf_repo()

    def init_csaf_repo(self) -> None:
        csaf_downloader = get_config('generic', 'csaf_downloader_path')
        if not csaf_downloader or not os.path.exists(csaf_downloader):
            self.logger.error('A CSAF downloader is required for this feeder')
            raise FileNotFoundError('CSAF downloader not found')

        self.path_to_repo = get_homedir() / 'vulnerabilitylookup' / 'feeders' / self.name
        if not self.path_to_repo.exists():
            self.pull_csaf_repo()

    def pull_csaf_repo(self, last_update: str | None = None) -> None:
        csaf_downloader = get_config('generic', 'csaf_downloader_path')
        if not csaf_downloader or not os.path.exists(csaf_downloader):
            self.logger.error('A CSAF downloader is required for this feeder')
            raise FileNotFoundError('CSAF downloader not found')

        if not hasattr(self, 'csaf_metadata'):
            self.logger.error('The URL to the metadata file is required for this feeder')
            raise Exception('Link to the metadata CSAF repository is missing.')

        to_run = [csaf_downloader, '-d', self.path_to_repo]
        if last_update:
            to_run.append('-t')
            to_run.append(last_update)
        to_run.append(self.csaf_metadata)
        with Popen(to_run) as process:
            self.logger.info('Downloading CSAF data.')
            while process.poll() is None:
                if last_update:
                    self.logger.info(f'Downloading CSAF data since {last_update}')
                else:
                    self.logger.info('Still downloading CSAF data...')
                time.sleep(10)
            self.logger.info('Downloading CSAF data finished')

    def update(self, stop: Callable[..., bool]) -> bool:
        # TODO: try to only get the updates. Not sure it is possible
        # but this link might help: https://github.com/csaf-poc/csaf_distribution/blob/main/docs/csaf_downloader.md#timerange-option
        self.init_csaf_repo()
        paths_to_import: set[Path] = set()

        update_time = datetime.now().isoformat()

        if _last_update := self.storage.hget('last_updates', self.name):
            # The source has already been imported, we just get the changes since last update
            _last_update_str = _last_update.decode()
            # Get last time the repo was updated
            if update_time > _last_update_str:
                self.logger.info(f'Pulling updates since {_last_update_str}.')
                self.pull_csaf_repo(_last_update_str)
            else:
                # No changes
                self.logger.info('No updates.')
                return False
        else:
            # No entry in last update, import all.
            self.pull_csaf_repo()

        paths_to_import = set()
        for tlp_color in self.path_to_repo.iterdir():
            if not tlp_color.is_dir():
                # That's the logfile
                continue
            for year in tlp_color.iterdir():
                for csaf_file in year.iterdir():
                    if re.match(self.file_pattern, csaf_file.name):
                        paths_to_import.add(csaf_file)

        if not paths_to_import:
            return False

        import_complete: bool = True
        p = self.storage.pipeline()
        csafids: dict[str, float] = {}
        vuln_to_push = []
        for path in paths_to_import:
            # Store all cves individually
            with path.open() as vuln_entry:
                vuln_str = vuln_entry.read()
                vuln_to_push.append(vuln_str)
                vuln = orjson.loads(vuln_str)
                modified = fromisoformat_wrapper(vuln['document']['tracking']['current_release_date'])
                vuln_id = path.stem.lower()
                csafids[vuln_id] = modified.timestamp()
                if 'vulnerabilities' in vuln and vuln.get('vulnerabilities'):
                    for _v in vuln.get('vulnerabilities'):
                        if cve := _v.get('cve'):
                            cve = cve.lower()
                            p.sadd(f'{vuln_id}:link', cve)
                            p.sadd(f'{cve}:link', vuln_id)
                p.set(vuln_id, orjson.dumps(vuln))
            if len(csafids) > 1000:
                # Avoid a massive execute on first import
                p.zadd(f'index:{self.name}', csafids)  # type: ignore
                p.zadd('index', csafids)  # type: ignore
                p.execute()

                # Publish the vulnerabilities
                self.publish(vuln_to_push)

                # reset pipeline
                p = self.storage.pipeline()
                csafids = {}

                # Reset the list of vulnerabilities to push
                vuln_to_push = []

            if stop():
                self.logger.info('Shutdown requested. Stopping import.')
                import_complete = False
                break

        if csafids:
            # remaining entries
            p.zadd(f'index:{self.name}', csafids)  # type: ignore
            p.zadd('index', csafids)  # type: ignore
            p.execute()

            # Publish the vulnerabilities
            self.publish(vuln_to_push)
            vuln_to_push = []

        if import_complete:
            self.storage.hset('last_updates', mapping={self.name: update_time})
            self.logger.info('Import done.')
            last_line = self.get_download_stats()
            self.logger.info(f'Download Statistics: {last_line}')
            return True
        return False

    def get_download_stats(self) -> str:
        log_file = self.path_to_repo / 'downloader.log'
        if not log_file.exists():
            return ''
        with log_file.open('rb') as f:
            try:
                f.seek(-2, os.SEEK_END)
                while f.read(1) != b'\n':
                    f.seek(-2, os.SEEK_CUR)
            except OSError:
                f.seek(0)
            last_line = json.loads(f.readline())
        return (str(last_line) if last_line else 'No stats found')
